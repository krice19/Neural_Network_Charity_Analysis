# Neural_Network_Charity_Analysis

## Overview

This analysis uses machine learning techniques on neural networks to help the Non profit foundation Alphabet Soup. Alphabet Soup has raise and donated over ten billion dollars in the past 20 years.  This analysis will help Alphabet Soup analyze the imapct of each donation to ensure the foundation's money is being used effectively.  I will use a deep learning neural network model on the provided CSV file to create a binary classifier that is capable of predicting whether applicants will be successful, with the goal of being able to predict which organization they should donate to and which ones are too high risk.  


## Results

The first step in the machine learning process is to preprocess the data set.  The target variable in the data set is the IS_SUCCESSFUL columns.  The columns APPLICATION_TYPE, AFFILIATION, CLASSIFICATION, USE_CAS, ORGANIZATION, STATUS, INCOME_AMT, SPECIAL_CONSIDERATIONS, and ASK_AMT were used as our features.  The two columns dropped in the dataframe were EIN and NAME since they are just identification columns.  We removed these from the input data since they would not provide any helpful statistical analysis to the algorithm.  X was defined as all our columns in the data frame whole dropping the IS_SUCCESSFUL column.  While building the neural network, I chose to have two hidden layers.  The first layer was defined with 80 neurons and the second layer was defined with 40 neurons.  Since this was a large data set, I wanted to include two hidden layers for better optimization.  I went with 80 and 40 neurons because a general rule of thumb is that have 2 or 3 times the amount nuerons in the hidden layers as the amount of input features in the model.  After encoding and bucketing, the data set resulted in 44 input features.  I chose the activation function "relu" for the hidden layers since we aree working with complex and nonlinear data for a classification model.  In my first attempt, I reached 72.59% accuracy, which is slighly below our target goal.  I made three more optimization attempts to increase the accuracy.  In the first attempt, I increase the hidden layer neurons to 160 and the second layer neurons to 80.  I also increase the epoch amount from 100 to 150.  This resulted in 72.52% accruary, which is slightly lower than the first model.  In my second optimization attempt, I addeed a third hidden layer, but lowered the amount of neurons in each.  My first hidden layer had 80 neurons, my second had 40 and my third had 20.   This time, I lowered the epoch amount back to 100.  This resulted in 72.37% accuracy, again lower than the first model.  In my last attempt, I decided to remove the columns for classification.  I thought this columns might contain noisy variables because the bucketed other columns contains greater amounts some already defined buckets.  I thought the variabilty was too high and this columns may not provide statistical information. I only added 2 hidden layers, one  with 114 and one with 76, and kept epochs at 100.  This resulted in 72.24% accuracy, which is the lowest scores out of all attempts.

## Summary

After four different model, I was not able to reach 75% accuracy.  Each optimization attempt resulted in a lower accuracy score than the previous, so my next steps would be to start at my first model and re-evaluate from there.  
A model that to help this classification analysis could be to try a new activation function such a tahn and re-evaluate the input data to eliminate unnessesary columns. 
